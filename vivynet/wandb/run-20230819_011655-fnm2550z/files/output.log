/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-08-19 01:17:33 | INFO | fairseq_cli.train | VIVYNet(
  (encoder): BERTBaseEN(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): SymphonyNetVanilla(
    (wEvte): Embedding(1125, 512)
    (wTrke): Embedding(44, 512)
    (wDure): Embedding(36, 512)
    (wRpe): Embedding(199, 512)
    (wMpe): Embedding(5361, 512)
    (drop): Dropout(p=0.1, inplace=False)
    (ln_f): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (proj_evt): Linear(in_features=512, out_features=1125, bias=False)
    (proj_dur): Linear(in_features=512, out_features=36, bias=False)
    (proj_trk): Linear(in_features=512, out_features=44, bias=False)
    (proj_ins): Linear(in_features=512, out_features=133, bias=False)
  )
  (intermediary): IntermediarySection(
    (linear_layers): Sequential(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.25, inplace=False)
      (3): ReLU()
      (4): Linear(in_features=768, out_features=512, bias=True)
      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (6): Dropout(p=0.25, inplace=False)
      (7): ReLU()
      (8): Linear(in_features=512, out_features=512, bias=True)
      (9): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (10): Dropout(p=0.25, inplace=False)
      (11): ReLU()
    )
  )
)
2023-08-19 01:17:33 | INFO | fairseq_cli.train | task: text2music (VIVYData)
2023-08-19 01:17:33 | INFO | fairseq_cli.train | model: vivy_transformer (VIVYNet)
2023-08-19 01:17:33 | INFO | fairseq_cli.train | criterion: nll_loss (ModelCriterion)
2023-08-19 01:17:33 | INFO | fairseq_cli.train | num. model params: 164160000 (num. trained: 55849728)
2023-08-19 01:17:33 | INFO | fairseq.trainer | detected shared parameter: decoder.proj_evt.bias <- decoder.proj_dur.bias
2023-08-19 01:17:33 | INFO | fairseq.trainer | detected shared parameter: decoder.proj_evt.bias <- decoder.proj_trk.bias
2023-08-19 01:17:33 | INFO | fairseq.trainer | detected shared parameter: decoder.proj_evt.bias <- decoder.proj_ins.bias
2023-08-19 01:17:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-08-19 01:17:33 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = NVIDIA GeForce GTX 1080 Ti
2023-08-19 01:17:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-08-19 01:17:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-08-19 01:17:33 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 1
2023-08-19 01:17:33 | INFO | fairseq.trainer | no existing checkpoint found ./results/default/ckpt/checkpoint_last.pt
2023-08-19 01:17:33 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-19 01:17:34 | INFO | fairseq.data.data_utils | loaded 956267 examples from: ../data/final/labels/bin/train
2023-08-19 01:17:34 | INFO | fairseq.data.data_utils | loaded 14162 examples from: ../data/final/features/train
2023-08-19 01:17:34 | INFO | fairseq.trainer | begin training epoch 1
2023-08-19 01:17:38 | INFO | train_inner | epoch 001:     10 / 14162 loss=4.467, evt_loss=9.72, dur_loss=2.252, trk_loss=2.916, ins_loss=2.978, ppl=22.11, evt_ppl=843.45, dur_ppl=4.76, trk_ppl=7.55, ins_ppl=7.88, wps=6248.8, ups=3.2, wpb=2050.3, bsz=1, num_updates=10, lr=9.9999e-06, gnorm=9.087, train_wall=4, wall=5
2023-08-19 01:17:42 | INFO | train_inner | epoch 001:     20 / 14162 loss=3.97, evt_loss=8.947, dur_loss=2.094, trk_loss=2.513, ins_loss=2.324, ppl=15.67, evt_ppl=493.65, dur_ppl=4.27, trk_ppl=5.71, ins_ppl=5.01, wps=4414.3, ups=2.82, wpb=1564.3, bsz=1, num_updates=20, lr=9.9998e-06, gnorm=7.062, train_wall=3, wall=8
2023-08-19 01:17:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.92 GiB total capacity; 9.89 GiB already allocated; 143.44 MiB free; 10.17 GiB reserved in total by PyTorch)
2023-08-19 01:17:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10132 MB |   10132 MB |  196118 MB |  185986 MB |
|       from large pool |    9801 MB |    9801 MB |  184564 MB |  174763 MB |
|       from small pool |     331 MB |     515 MB |   11553 MB |   11222 MB |
|---------------------------------------------------------------------------|
| Active memory         |   10132 MB |   10132 MB |  196118 MB |  185986 MB |
|       from large pool |    9801 MB |    9801 MB |  184564 MB |  174763 MB |
|       from small pool |     331 MB |     515 MB |   11553 MB |   11222 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10414 MB |   10524 MB |   30496 MB |   20082 MB |
|       from large pool |   10078 MB |   10078 MB |   29626 MB |   19548 MB |
|       from small pool |     336 MB |     542 MB |     870 MB |     534 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  288303 KB |    1404 MB |  142031 MB |  141749 MB |
|       from large pool |  283194 KB |    1401 MB |  130199 MB |  129922 MB |
|       from small pool |    5109 KB |      64 MB |   11832 MB |   11827 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1597    |    1921    |   76607    |   75010    |
|       from large pool |     405    |     497    |   22693    |   22288    |
|       from small pool |    1192    |    1730    |   53914    |   52722    |
|---------------------------------------------------------------------------|
| Active allocs         |    1597    |    1921    |   76607    |   75010    |
|       from large pool |     405    |     497    |   22693    |   22288    |
|       from small pool |    1192    |    1730    |   53914    |   52722    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     332    |     473    |     955    |     623    |
|       from large pool |     164    |     207    |     520    |     356    |
|       from small pool |     168    |     271    |     435    |     267    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |     206    |   37292    |   37194    |
|       from large pool |      69    |     114    |   15039    |   14970    |
|       from small pool |      29    |     142    |   22253    |   22224    |
|===========================================================================|
2023-08-19 01:17:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:17:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 10.92 GiB total capacity; 7.79 GiB already allocated; 141.44 MiB free; 10.17 GiB reserved in total by PyTorch)
2023-08-19 01:17:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    7977 MB |   10132 MB |  266199 MB |  258221 MB |
|       from large pool |    7651 MB |    9801 MB |  251689 MB |  244037 MB |
|       from small pool |     326 MB |     515 MB |   14510 MB |   14184 MB |
|---------------------------------------------------------------------------|
| Active memory         |    7977 MB |   10132 MB |  266199 MB |  258221 MB |
|       from large pool |    7651 MB |    9801 MB |  251689 MB |  244037 MB |
|       from small pool |     326 MB |     515 MB |   14510 MB |   14184 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10416 MB |   10534 MB |   30958 MB |   20542 MB |
|       from large pool |   10086 MB |   10086 MB |   29966 MB |   19880 MB |
|       from small pool |     330 MB |     542 MB |     992 MB |     662 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2438 MB |    2664 MB |  210907 MB |  208469 MB |
|       from large pool |    2434 MB |    2659 MB |  196002 MB |  193568 MB |
|       from small pool |       3 MB |      64 MB |   14904 MB |   14901 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1633    |    1921    |   98994    |   97361    |
|       from large pool |     431    |     501    |   30151    |   29720    |
|       from small pool |    1202    |    1730    |   68843    |   67641    |
|---------------------------------------------------------------------------|
| Active allocs         |    1633    |    1921    |   98994    |   97361    |
|       from large pool |     431    |     501    |   30151    |   29720    |
|       from small pool |    1202    |    1730    |   68843    |   67641    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     325    |     473    |    1018    |     693    |
|       from large pool |     160    |     207    |     522    |     362    |
|       from small pool |     165    |     271    |     496    |     331    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     152    |     206    |   48347    |   48195    |
|       from large pool |     115    |     118    |   19960    |   19845    |
|       from small pool |      37    |     142    |   28387    |   28350    |
|===========================================================================|
2023-08-19 01:17:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:17:46 | INFO | train_inner | epoch 001:     32 / 14162 loss=3.64, evt_loss=8.443, dur_loss=1.636, trk_loss=2.553, ins_loss=1.926, ppl=12.46, evt_ppl=348, dur_ppl=3.11, trk_ppl=5.87, ins_ppl=3.8, wps=3737.1, ups=2.38, wpb=1568.1, bsz=1, num_updates=30, lr=9.9997e-06, gnorm=6.816, train_wall=3, wall=13
2023-08-19 01:17:50 | INFO | train_inner | epoch 001:     42 / 14162 loss=3.936, evt_loss=8.497, dur_loss=1.858, trk_loss=2.749, ins_loss=2.64, ppl=15.31, evt_ppl=361.41, dur_ppl=3.62, trk_ppl=6.72, ins_ppl=6.23, wps=3986.2, ups=2.4, wpb=1662.7, bsz=1, num_updates=40, lr=9.9996e-06, gnorm=6.579, train_wall=4, wall=17
2023-08-19 01:17:53 | INFO | train_inner | epoch 001:     52 / 14162 loss=3.374, evt_loss=7.877, dur_loss=1.605, trk_loss=2.373, ins_loss=1.641, ppl=10.37, evt_ppl=235.04, dur_ppl=3.04, trk_ppl=5.18, ins_ppl=3.12, wps=3834.1, ups=3.21, wpb=1194.5, bsz=1, num_updates=50, lr=9.9995e-06, gnorm=6.303, train_wall=3, wall=20
2023-08-19 01:17:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.92 GiB total capacity; 9.89 GiB already allocated; 151.44 MiB free; 10.16 GiB reserved in total by PyTorch)
2023-08-19 01:17:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10132 MB |   10132 MB |  473147 MB |  463014 MB |
|       from large pool |    9801 MB |    9801 MB |  447828 MB |  438026 MB |
|       from small pool |     331 MB |     515 MB |   25319 MB |   24988 MB |
|---------------------------------------------------------------------------|
| Active memory         |   10132 MB |   10132 MB |  473147 MB |  463014 MB |
|       from large pool |    9801 MB |    9801 MB |  447828 MB |  438026 MB |
|       from small pool |     331 MB |     515 MB |   25319 MB |   24988 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10406 MB |   10554 MB |   49918 MB |   39512 MB |
|       from large pool |   10070 MB |   10106 MB |   48628 MB |   38558 MB |
|       from small pool |     336 MB |     542 MB |    1290 MB |     954 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  280111 KB |    2664 MB |  401899 MB |  401626 MB |
|       from large pool |  275002 KB |    2659 MB |  375861 MB |  375593 MB |
|       from small pool |    5109 KB |      66 MB |   26037 MB |   26032 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1597    |    1921    |  185101    |  183504    |
|       from large pool |     405    |     501    |   59774    |   59369    |
|       from small pool |    1192    |    1730    |  125327    |  124135    |
|---------------------------------------------------------------------------|
| Active allocs         |    1597    |    1921    |  185101    |  183504    |
|       from large pool |     405    |     501    |   59774    |   59369    |
|       from small pool |    1192    |    1730    |  125327    |  124135    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     331    |     473    |    1424    |    1093    |
|       from large pool |     163    |     207    |     779    |     616    |
|       from small pool |     168    |     271    |     645    |     477    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     210    |   90726    |   90626    |
|       from large pool |      70    |     119    |   40505    |   40435    |
|       from small pool |      30    |     171    |   50221    |   50191    |
|===========================================================================|
2023-08-19 01:17:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:17:57 | INFO | train_inner | epoch 001:     63 / 14162 loss=3.374, evt_loss=7.52, dur_loss=1.389, trk_loss=2.519, ins_loss=2.068, ppl=10.37, evt_ppl=183.6, dur_ppl=2.62, trk_ppl=5.73, ins_ppl=4.19, wps=4572, ups=2.9, wpb=1578.8, bsz=1, num_updates=60, lr=9.9994e-06, gnorm=5.447, train_wall=3, wall=23
2023-08-19 01:18:00 | INFO | train_inner | epoch 001:     73 / 14162 loss=3.764, evt_loss=7.668, dur_loss=1.711, trk_loss=2.781, ins_loss=2.896, ppl=13.59, evt_ppl=203.32, dur_ppl=3.27, trk_ppl=6.87, ins_ppl=7.45, wps=5029.1, ups=3.01, wpb=1668.6, bsz=1, num_updates=70, lr=9.9993e-06, gnorm=5.947, train_wall=3, wall=27
2023-08-19 01:18:04 | INFO | train_inner | epoch 001:     83 / 14162 loss=3.368, evt_loss=7.184, dur_loss=1.472, trk_loss=2.429, ins_loss=2.386, ppl=10.32, evt_ppl=145.38, dur_ppl=2.77, trk_ppl=5.38, ins_ppl=5.23, wps=5567.3, ups=2.55, wpb=2180, bsz=1, num_updates=80, lr=9.9992e-06, gnorm=5.358, train_wall=4, wall=31
2023-08-19 01:18:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.92 GiB total capacity; 9.89 GiB already allocated; 133.44 MiB free; 10.18 GiB reserved in total by PyTorch)
2023-08-19 01:18:04 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10132 MB |   10132 MB |  763157 MB |  753024 MB |
|       from large pool |    9801 MB |    9801 MB |  724208 MB |  714407 MB |
|       from small pool |     331 MB |     515 MB |   38948 MB |   38617 MB |
|---------------------------------------------------------------------------|
| Active memory         |   10132 MB |   10132 MB |  763157 MB |  753024 MB |
|       from large pool |    9801 MB |    9801 MB |  724208 MB |  714407 MB |
|       from small pool |     331 MB |     515 MB |   38948 MB |   38617 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10424 MB |   10554 MB |   73034 MB |   62610 MB |
|       from large pool |   10090 MB |   10106 MB |   71292 MB |   61202 MB |
|       from small pool |     334 MB |     550 MB |    1742 MB |    1408 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  298546 KB |    2664 MB |  657806 MB |  657514 MB |
|       from large pool |  295482 KB |    2659 MB |  617708 MB |  617419 MB |
|       from small pool |    3064 KB |      66 MB |   40097 MB |   40094 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1591    |    1921    |  289189    |  287598    |
|       from large pool |     405    |     501    |   93658    |   93253    |
|       from small pool |    1186    |    1730    |  195531    |  194345    |
|---------------------------------------------------------------------------|
| Active allocs         |    1591    |    1921    |  289189    |  287598    |
|       from large pool |     405    |     501    |   93658    |   93253    |
|       from small pool |    1186    |    1730    |  195531    |  194345    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     336    |     514    |    2032    |    1696    |
|       from large pool |     169    |     239    |    1161    |     992    |
|       from small pool |     167    |     275    |     871    |     704    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |     215    |  141857    |  141761    |
|       from large pool |      73    |     147    |   63557    |   63484    |
|       from small pool |      23    |     171    |   78300    |   78277    |
|===========================================================================|
2023-08-19 01:18:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:18:07 | INFO | train_inner | epoch 001:     94 / 14162 loss=2.835, evt_loss=6.693, dur_loss=1.382, trk_loss=2.265, ins_loss=1.001, ppl=7.14, evt_ppl=103.46, dur_ppl=2.61, trk_ppl=4.81, ins_ppl=2, wps=2470.5, ups=3.24, wpb=762.1, bsz=1, num_updates=90, lr=9.9991e-06, gnorm=5.225, train_wall=3, wall=34
2023-08-19 01:18:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 10.92 GiB total capacity; 9.79 GiB already allocated; 51.44 MiB free; 10.26 GiB reserved in total by PyTorch)
2023-08-19 01:18:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9808 MB |   10132 MB |     878 GB |     868 GB |
|       from large pool |    9475 MB |    9801 MB |     830 GB |     821 GB |
|       from small pool |     332 MB |     520 MB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| Active memory         |    9808 MB |   10132 MB |     878 GB |     868 GB |
|       from large pool |    9475 MB |    9801 MB |     830 GB |     821 GB |
|       from small pool |     332 MB |     520 MB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10506 MB |   10554 MB |  100590 MB |   90084 MB |
|       from large pool |   10170 MB |   10170 MB |   98432 MB |   88262 MB |
|       from small pool |     336 MB |     554 MB |    2158 MB |    1822 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  487344 KB |    2664 MB |  745843 MB |  745367 MB |
|       from large pool |  483797 KB |    2659 MB |  695522 MB |  695050 MB |
|       from small pool |    3547 KB |      90 MB |   50321 MB |   50317 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1634    |    1921    |  349602    |  347968    |
|       from large pool |     432    |     501    |  110822    |  110390    |
|       from small pool |    1202    |    1730    |  238780    |  237578    |
|---------------------------------------------------------------------------|
| Active allocs         |    1634    |    1921    |  349602    |  347968    |
|       from large pool |     432    |     501    |  110822    |  110390    |
|       from small pool |    1202    |    1730    |  238780    |  237578    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     346    |     514    |    2637    |    2291    |
|       from large pool |     178    |     239    |    1558    |    1380    |
|       from small pool |     168    |     277    |    1079    |     911    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     133    |     219    |  173256    |  173123    |
|       from large pool |     102    |     147    |   75718    |   75616    |
|       from small pool |      31    |     176    |   97538    |   97507    |
|===========================================================================|
2023-08-19 01:18:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:18:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.92 GiB total capacity; 9.89 GiB already allocated; 177.44 MiB free; 10.14 GiB reserved in total by PyTorch)
2023-08-19 01:18:11 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10132 MB |   10132 MB |     920 GB |     910 GB |
|       from large pool |    9801 MB |    9801 MB |     872 GB |     862 GB |
|       from small pool |     331 MB |     520 MB |      48 GB |      48 GB |
|---------------------------------------------------------------------------|
| Active memory         |   10132 MB |   10132 MB |     920 GB |     910 GB |
|       from large pool |    9801 MB |    9801 MB |     872 GB |     862 GB |
|       from small pool |     331 MB |     520 MB |      48 GB |      48 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10380 MB |   10554 MB |  109742 MB |   99362 MB |
|       from large pool |   10046 MB |   10170 MB |  107488 MB |   97442 MB |
|       from small pool |     334 MB |     554 MB |    2254 MB |    1920 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  253487 KB |    2664 MB |  781992 MB |  781744 MB |
|       from large pool |  250426 KB |    2659 MB |  730774 MB |  730530 MB |
|       from small pool |    3061 KB |      90 MB |   51217 MB |   51214 MB |
|---------------------------------------------------------------------------|
| Allocations           |    1597    |    1921    |  357792    |  356195    |
|       from large pool |     405    |     501    |  114078    |  113673    |
|       from small pool |    1192    |    1730    |  243714    |  242522    |
|---------------------------------------------------------------------------|
| Active allocs         |    1597    |    1921    |  357792    |  356195    |
|       from large pool |     405    |     501    |  114078    |  113673    |
|       from small pool |    1192    |    1730    |  243714    |  242522    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     330    |     514    |    2799    |    2469    |
|       from large pool |     163    |     239    |    1672    |    1509    |
|       from small pool |     167    |     277    |    1127    |     960    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |     219    |  177266    |  177180    |
|       from large pool |      65    |     147    |   77740    |   77675    |
|       from small pool |      21    |     176    |   99526    |   99505    |
|===========================================================================|
2023-08-19 01:18:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:18:11 | INFO | train_inner | epoch 001:    106 / 14162 loss=3.315, evt_loss=7.142, dur_loss=1.518, trk_loss=2.39, ins_loss=2.212, ppl=9.95, evt_ppl=141.2, dur_ppl=2.86, trk_ppl=5.24, ins_ppl=4.63, wps=5008.5, ups=2.16, wpb=2320.5, bsz=1, num_updates=100, lr=9.999e-06, gnorm=5.877, train_wall=4, wall=38
2023-08-19 01:18:15 | INFO | train_inner | epoch 001:    116 / 14162 loss=3.43, evt_loss=7.493, dur_loss=1.87, trk_loss=2.655, ins_loss=1.703, ppl=10.78, evt_ppl=180.11, dur_ppl=3.65, trk_ppl=6.3, ins_ppl=3.26, wps=4450.3, ups=3.11, wpb=1430.8, bsz=1, num_updates=110, lr=9.9989e-06, gnorm=5.595, train_wall=3, wall=42
2023-08-19 01:18:17 | INFO | train_inner | epoch 001:    126 / 14162 loss=3.16, evt_loss=6.919, dur_loss=1.681, trk_loss=2.348, ins_loss=1.692, ppl=8.94, evt_ppl=121.05, dur_ppl=3.21, trk_ppl=5.09, ins_ppl=3.23, wps=4869.7, ups=3.69, wpb=1321, bsz=1, num_updates=120, lr=9.9988e-06, gnorm=6.066, train_wall=3, wall=44
2023-08-19 01:18:21 | INFO | train_inner | epoch 001:    136 / 14162 loss=3.275, evt_loss=7.11, dur_loss=1.672, trk_loss=2.41, ins_loss=1.908, ppl=9.68, evt_ppl=138.17, dur_ppl=3.19, trk_ppl=5.32, ins_ppl=3.75, wps=5050.4, ups=3.12, wpb=1616.2, bsz=1, num_updates=130, lr=9.9987e-06, gnorm=5.056, train_wall=3, wall=47
2023-08-19 01:18:24 | INFO | train_inner | epoch 001:    146 / 14162 loss=3.161, evt_loss=6.97, dur_loss=1.442, trk_loss=2.387, ins_loss=1.846, ppl=8.95, evt_ppl=125.39, dur_ppl=2.72, trk_ppl=5.23, ins_ppl=3.59, wps=4898, ups=2.77, wpb=1768.2, bsz=1, num_updates=140, lr=9.9986e-06, gnorm=5.12, train_wall=3, wall=51
2023-08-19 01:18:28 | INFO | train_inner | epoch 001:    156 / 14162 loss=3.308, evt_loss=6.985, dur_loss=1.651, trk_loss=2.574, ins_loss=2.022, ppl=9.9, evt_ppl=126.67, dur_ppl=3.14, trk_ppl=5.95, ins_ppl=4.06, wps=5614.8, ups=2.4, wpb=2343, bsz=1, num_updates=150, lr=9.9985e-06, gnorm=5.279, train_wall=4, wall=55
2023-08-19 01:18:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.92 GiB total capacity; 9.89 GiB already allocated; 169.44 MiB free; 10.14 GiB reserved in total by PyTorch)
2023-08-19 01:18:31 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10132 MB |   10132 MB |    1463 GB |    1453 GB |
|       from large pool |    9801 MB |    9801 MB |    1386 GB |    1376 GB |
|       from small pool |     331 MB |     543 MB |      76 GB |      76 GB |
|---------------------------------------------------------------------------|
| Active memory         |   10132 MB |   10132 MB |    1463 GB |    1453 GB |
|       from large pool |    9801 MB |    9801 MB |    1386 GB |    1376 GB |
|       from small pool |     331 MB |     543 MB |      76 GB |      76 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   10388 MB |   10554 MB |  140870 MB |  130482 MB |
|       from large pool |   10052 MB |   10170 MB |  138036 MB |  127984 MB |
|       from small pool |     336 MB |     558 MB |    2834 MB |    2498 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  261679 KB |    2913 MB |    1223 GB |    1223 GB |
|       from large pool |  256570 KB |    2911 MB |    1144 GB |    1143 GB |
|       from small pool |    5109 KB |      90 MB |      79 GB |      79 GB |
|---------------------------------------------------------------------------|
| Allocations           |    1597    |    1921    |  565956    |  564359    |
|       from large pool |     405    |     501    |  180511    |  180106    |
|       from small pool |    1192    |    1730    |  385445    |  384253    |
|---------------------------------------------------------------------------|
| Active allocs         |    1597    |    1921    |  565956    |  564359    |
|       from large pool |     405    |     501    |  180511    |  180106    |
|       from small pool |    1192    |    1730    |  385445    |  384253    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     332    |     514    |    3535    |    3203    |
|       from large pool |     164    |     239    |    2118    |    1954    |
|       from small pool |     168    |     279    |    1417    |    1249    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |     244    |  279384    |  279291    |
|       from large pool |      67    |     147    |  123244    |  123177    |
|       from small pool |      26    |     192    |  156140    |  156114    |
|===========================================================================|
2023-08-19 01:18:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2023-08-19 01:18:32 | INFO | train_inner | epoch 001:    167 / 14162 loss=2.905, evt_loss=6.526, dur_loss=1.474, trk_loss=2.352, ins_loss=1.268, ppl=7.49, evt_ppl=92.17, dur_ppl=2.78, trk_ppl=5.11, ins_ppl=2.41, wps=4944.2, ups=2.92, wpb=1692.4, bsz=1, num_updates=160, lr=9.9984e-06, gnorm=4.848, train_wall=3, wall=59
2023-08-19 01:18:35 | INFO | train_inner | epoch 001:    177 / 14162 loss=3.227, evt_loss=6.591, dur_loss=1.575, trk_loss=2.342, ins_loss=2.402, ppl=9.37, evt_ppl=96.44, dur_ppl=2.98, trk_ppl=5.07, ins_ppl=5.29, wps=5436.7, ups=2.95, wpb=1843.3, bsz=1, num_updates=170, lr=9.9983e-06, gnorm=4.748, train_wall=3, wall=62
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/tokenize.py", line 394, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/tokenize.py", line 363, in detect_encoding
    first = read_or_stop()
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/tokenize.py", line 321, in read_or_stop
    return readline()
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "/home/blherre4/.conda/envs/vivyenv/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/distributed_utils.py", line 301, in call_main
    main(args, **kwargs)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq_cli/train.py", line 125, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq_cli/train.py", line 208, in train
    log_output = trainer.train_step(samples)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/trainer.py", line 580, in train_step
    grad_norm = self.clip_grad_norm(self.args.clip_norm)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/trainer.py", line 855, in clip_grad_norm
    return self.optimizer.clip_grad_norm(clip_norm, aggregate_norm_fn=None)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/optim/fairseq_optimizer.py", line 105, in clip_grad_norm
    return utils.clip_grad_norm_(self.params, max_norm, aggregate_norm_fn)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/utils.py", line 352, in clip_grad_norm_
    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/fairseq/utils.py", line 352, in <listcomp>
    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]
  File "/home/blherre4/.conda/envs/vivyenv/lib/python3.8/site-packages/torch/functional.py", line 1280, in norm
    if type(input) is not Tensor and has_torch_function((input,)):
KeyboardInterrupt