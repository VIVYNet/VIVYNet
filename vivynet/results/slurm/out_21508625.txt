Purging modules
Loading python 3 from anaconda module
Loading VIVYNET conda environment
Showing GPU details
GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-d1efc34b-180a-b9df-6350-3ee609bddc5a)
Tue Jun 27 18:44:47 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:1A:00.0 Off |                    0 |
| N/A   43C    P0    55W / 300W |      0MiB / 16384MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Running training python script
2023-06-27 18:46:05 | INFO | fairseq_cli.train | Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, all_gather_list_size=16384, arch='vivy_train', batch_size=1, batch_size_valid=1, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='nll_loss', curriculum=0, data='../data/final', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', dec_dropout=0.1, dec_embed_dim=512, dec_num_attention_heads=16, dec_num_layers=12, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dur_voc_size=36, empty_cache_freq=0, evt_voc_size=1125, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, freeze_dec=1, freeze_enc=1, gen_subset='test', ins_voc_size=133, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_mea_pos=5360, max_rel_pos=198, max_tokens=8192, max_tokens_valid=8192, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, perm_inv=2, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quantization_config_path=None, ratio=4, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete_doc', sample_overlap_rate=4, save_dir='./results/ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1998, sentence_avg=False, shard_id=0, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='text2music', tensorboard_logdir='./results/logs', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=4096, tpu=False, train_subset='train', trk_voc_size=44, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir='./', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')
| [input] dictionary: 16312 types
| [label] dictionary: 380 types
2023-06-27 18:46:21 | INFO | fairseq.data.data_utils | loaded 2084234 examples from: ../data/final/labels/bin/valid
2023-06-27 18:46:22 | INFO | fairseq.data.data_utils | loaded 26788 examples from: ../data/final/features/valid
2023-06-27 18:47:23 | INFO | fairseq_cli.train | VIVYNet(
  (encoder): BERT(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(119547, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): SymphonyNet(
    (wEvte): Embedding(1125, 512)
    (wTrke): Embedding(44, 512)
    (wDure): Embedding(36, 512)
    (wRpe): Embedding(199, 512)
    (wMpe): Embedding(5361, 512)
    (drop): Dropout(p=0.1, inplace=False)
    (ln_f): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): TransformerDecoderLayer(
          (self_attention): AttentionLayer(
            (inner_attention): CausalLinearAttention(
              (feature_map): ActivationFunctionFeatureMap()
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (cross_attention): AttentionLayer(
            (inner_attention): FullAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (key_projection): Linear(in_features=512, out_features=512, bias=True)
            (value_projection): Linear(in_features=512, out_features=512, bias=True)
            (out_projection): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (proj_evt): Linear(in_features=512, out_features=1125, bias=False)
    (proj_dur): Linear(in_features=512, out_features=36, bias=False)
    (proj_trk): Linear(in_features=512, out_features=44, bias=False)
    (proj_ins): Linear(in_features=512, out_features=133, bias=False)
  )
  (linear): Linear(in_features=768, out_features=512, bias=True)
)
2023-06-27 18:47:23 | INFO | fairseq_cli.train | task: text2music (VIVYData)
2023-06-27 18:47:23 | INFO | fairseq_cli.train | model: vivy_train (VIVYNet)
2023-06-27 18:47:23 | INFO | fairseq_cli.train | criterion: nll_loss (ModelCriterion)
2023-06-27 18:47:23 | INFO | fairseq_cli.train | num. model params: 232846336 (num. trained: 42385408)
2023-06-27 18:47:23 | INFO | fairseq.trainer | detected shared parameter: decoder.proj_evt.bias <- decoder.proj_dur.bias
2023-06-27 18:47:23 | INFO | fairseq.trainer | detected shared parameter: decoder.proj_evt.bias <- decoder.proj_trk.bias
2023-06-27 18:47:23 | INFO | fairseq.trainer | detected shared parameter: decoder.proj_evt.bias <- decoder.proj_ins.bias
2023-06-27 18:47:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-06-27 18:47:23 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-SXM2-16GB                    
2023-06-27 18:47:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-06-27 18:47:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-06-27 18:47:23 | INFO | fairseq_cli.train | max tokens per GPU = 8192 and max sentences per GPU = 1
2023-06-27 18:47:23 | INFO | fairseq.trainer | no existing checkpoint found ./results/ckpt/checkpoint_last.pt
2023-06-27 18:47:23 | INFO | fairseq.trainer | loading train data for epoch 1
2023-06-27 18:47:44 | INFO | fairseq.data.data_utils | loaded 2821420 examples from: ../data/final/labels/bin/train
2023-06-27 18:47:45 | INFO | fairseq.data.data_utils | loaded 40180 examples from: ../data/final/features/train
2023-06-27 18:47:45 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2023-06-27 18:47:45 | INFO | fairseq.trainer | begin training epoch 1
2023-06-27 18:48:11 | INFO | train_inner | epoch 001:    100 / 40180 loss=3.28, ppl=9.71, wps=7349.7, ups=4.09, wpb=1818.2, bsz=1, num_updates=100, lr=0.001, gnorm=3.442, train_wall=25, wall=48
2023-06-27 18:48:34 | INFO | train_inner | epoch 001:    200 / 40180 loss=2.934, ppl=7.64, wps=7169.9, ups=4.21, wpb=1703.5, bsz=1, num_updates=200, lr=0.001, gnorm=2.072, train_wall=23, wall=72
2023-06-27 18:48:59 | INFO | train_inner | epoch 001:    300 / 40180 loss=2.713, ppl=6.56, wps=7772.9, ups=4.13, wpb=1880.8, bsz=1, num_updates=300, lr=0.001, gnorm=1.695, train_wall=24, wall=96
2023-06-27 18:49:24 | INFO | train_inner | epoch 001:    400 / 40180 loss=2.727, ppl=6.62, wps=8477.9, ups=3.99, wpb=2125.2, bsz=1, num_updates=400, lr=0.001, gnorm=1.498, train_wall=25, wall=121
2023-06-27 18:49:48 | INFO | train_inner | epoch 001:    500 / 40180 loss=2.812, ppl=7.02, wps=7196.5, ups=4.15, wpb=1734, bsz=1, num_updates=500, lr=0.001, gnorm=1.471, train_wall=24, wall=145
2023-06-27 18:50:11 | INFO | train_inner | epoch 001:    600 / 40180 loss=2.776, ppl=6.85, wps=7152.5, ups=4.27, wpb=1675.2, bsz=1, num_updates=600, lr=0.001, gnorm=1.405, train_wall=23, wall=168
2023-06-27 18:50:36 | INFO | train_inner | epoch 001:    700 / 40180 loss=2.757, ppl=6.76, wps=7144.1, ups=4.06, wpb=1757.6, bsz=1, num_updates=700, lr=0.001, gnorm=1.384, train_wall=24, wall=193
2023-06-27 18:51:00 | INFO | train_inner | epoch 001:    800 / 40180 loss=2.766, ppl=6.8, wps=7510.2, ups=4.17, wpb=1801.3, bsz=1, num_updates=800, lr=0.001, gnorm=1.294, train_wall=23, wall=217
2023-06-27 18:51:25 | INFO | train_inner | epoch 001:    900 / 40180 loss=2.778, ppl=6.86, wps=8090.9, ups=3.99, wpb=2026.3, bsz=1, num_updates=900, lr=0.001, gnorm=1.268, train_wall=24, wall=242
2023-06-27 18:51:50 | INFO | train_inner | epoch 001:   1000 / 40180 loss=2.774, ppl=6.84, wps=7531.7, ups=4.03, wpb=1870.8, bsz=1, num_updates=1000, lr=0.001, gnorm=1.154, train_wall=24, wall=267
2023-06-27 18:52:14 | INFO | train_inner | epoch 001:   1100 / 40180 loss=2.747, ppl=6.71, wps=7466.9, ups=4.07, wpb=1833.5, bsz=1, num_updates=1100, lr=0.001, gnorm=1.106, train_wall=24, wall=291
2023-06-27 18:52:38 | INFO | train_inner | epoch 001:   1200 / 40180 loss=2.74, ppl=6.68, wps=7502.7, ups=4.23, wpb=1773.6, bsz=1, num_updates=1200, lr=0.001, gnorm=1.155, train_wall=23, wall=315
2023-06-27 18:53:01 | INFO | train_inner | epoch 001:   1300 / 40180 loss=2.732, ppl=6.64, wps=7549, ups=4.26, wpb=1773.2, bsz=1, num_updates=1300, lr=0.001, gnorm=1.046, train_wall=23, wall=338
2023-06-27 18:53:25 | INFO | train_inner | epoch 001:   1400 / 40180 loss=2.661, ppl=6.32, wps=8452.3, ups=4.25, wpb=1986.6, bsz=1, num_updates=1400, lr=0.001, gnorm=0.962, train_wall=23, wall=362
2023-06-27 18:53:49 | INFO | train_inner | epoch 001:   1500 / 40180 loss=2.662, ppl=6.33, wps=7640.3, ups=4.14, wpb=1846.2, bsz=1, num_updates=1500, lr=0.001, gnorm=0.991, train_wall=24, wall=386
2023-06-27 18:54:13 | INFO | train_inner | epoch 001:   1600 / 40180 loss=2.668, ppl=6.35, wps=7679.9, ups=4.15, wpb=1851.1, bsz=1, num_updates=1600, lr=0.001, gnorm=1.016, train_wall=24, wall=410
2023-06-27 18:54:38 | INFO | train_inner | epoch 001:   1700 / 40180 loss=2.69, ppl=6.45, wps=8074.2, ups=4.11, wpb=1963.7, bsz=1, num_updates=1700, lr=0.001, gnorm=0.967, train_wall=24, wall=435
2023-06-27 18:55:02 | INFO | train_inner | epoch 001:   1800 / 40180 loss=2.75, ppl=6.73, wps=8286.5, ups=4.05, wpb=2047.8, bsz=1, num_updates=1800, lr=0.001, gnorm=0.943, train_wall=24, wall=459
2023-06-27 18:55:27 | INFO | train_inner | epoch 001:   1900 / 40180 loss=2.733, ppl=6.65, wps=7718.3, ups=4.1, wpb=1884.8, bsz=1, num_updates=1900, lr=0.001, gnorm=0.963, train_wall=24, wall=484
2023-06-27 18:55:51 | INFO | train_inner | epoch 001:   2000 / 40180 loss=2.715, ppl=6.57, wps=7807.8, ups=4.09, wpb=1909.2, bsz=1, num_updates=2000, lr=0.001, gnorm=0.908, train_wall=24, wall=508
2023-06-27 18:56:15 | INFO | train_inner | epoch 001:   2100 / 40180 loss=2.767, ppl=6.8, wps=7615.8, ups=4.18, wpb=1822.6, bsz=1, num_updates=2100, lr=0.001, gnorm=0.917, train_wall=23, wall=532
2023-06-27 18:56:40 | INFO | train_inner | epoch 001:   2200 / 40180 loss=2.808, ppl=7, wps=6860.6, ups=4.02, wpb=1707.7, bsz=1, num_updates=2200, lr=0.001, gnorm=0.88, train_wall=24, wall=557
2023-06-27 18:57:04 | INFO | train_inner | epoch 001:   2300 / 40180 loss=2.743, ppl=6.69, wps=7602.9, ups=4.1, wpb=1852.8, bsz=1, num_updates=2300, lr=0.001, gnorm=0.815, train_wall=24, wall=581
2023-06-27 18:57:29 | INFO | train_inner | epoch 001:   2400 / 40180 loss=2.684, ppl=6.43, wps=7865.7, ups=4.05, wpb=1942.8, bsz=1, num_updates=2400, lr=0.001, gnorm=0.793, train_wall=24, wall=606
2023-06-27 18:57:53 | INFO | train_inner | epoch 001:   2500 / 40180 loss=2.756, ppl=6.76, wps=8169.9, ups=4.13, wpb=1978.2, bsz=1, num_updates=2500, lr=0.001, gnorm=0.819, train_wall=24, wall=630
2023-06-27 18:58:18 | INFO | train_inner | epoch 001:   2600 / 40180 loss=2.735, ppl=6.66, wps=7545.5, ups=4.03, wpb=1871.6, bsz=1, num_updates=2600, lr=0.001, gnorm=0.856, train_wall=24, wall=655
2023-06-27 18:58:42 | INFO | train_inner | epoch 001:   2700 / 40180 loss=2.682, ppl=6.42, wps=7833.1, ups=4.14, wpb=1893.9, bsz=1, num_updates=2700, lr=0.001, gnorm=0.798, train_wall=24, wall=679
2023-06-27 18:59:06 | INFO | train_inner | epoch 001:   2800 / 40180 loss=2.716, ppl=6.57, wps=7389.2, ups=4.16, wpb=1776.1, bsz=1, num_updates=2800, lr=0.001, gnorm=0.784, train_wall=23, wall=703
2023-06-27 18:59:31 | INFO | train_inner | epoch 001:   2900 / 40180 loss=2.656, ppl=6.3, wps=7296.6, ups=4.09, wpb=1783.2, bsz=1, num_updates=2900, lr=0.001, gnorm=0.748, train_wall=24, wall=728
2023-06-27 18:59:55 | INFO | train_inner | epoch 001:   3000 / 40180 loss=2.635, ppl=6.21, wps=8198.9, ups=4.07, wpb=2013.6, bsz=1, num_updates=3000, lr=0.001, gnorm=0.748, train_wall=24, wall=752
2023-06-27 19:00:20 | INFO | train_inner | epoch 001:   3100 / 40180 loss=2.715, ppl=6.57, wps=8035.9, ups=4.08, wpb=1970.4, bsz=1, num_updates=3100, lr=0.001, gnorm=0.751, train_wall=24, wall=777
2023-06-27 19:00:44 | INFO | train_inner | epoch 001:   3200 / 40180 loss=2.718, ppl=6.58, wps=7772.4, ups=4.08, wpb=1904.2, bsz=1, num_updates=3200, lr=0.001, gnorm=0.71, train_wall=24, wall=801
2023-06-27 19:01:09 | INFO | train_inner | epoch 001:   3300 / 40180 loss=2.659, ppl=6.32, wps=8211.4, ups=4.1, wpb=2001.5, bsz=1, num_updates=3300, lr=0.001, gnorm=0.698, train_wall=24, wall=826
2023-06-27 19:01:33 | INFO | train_inner | epoch 001:   3400 / 40180 loss=2.728, ppl=6.63, wps=7801.4, ups=4.03, wpb=1936.3, bsz=1, num_updates=3400, lr=0.001, gnorm=0.711, train_wall=24, wall=851
2023-06-27 19:01:58 | INFO | train_inner | epoch 001:   3500 / 40180 loss=2.639, ppl=6.23, wps=7759.2, ups=4.14, wpb=1875.9, bsz=1, num_updates=3500, lr=0.001, gnorm=0.66, train_wall=24, wall=875
