    # def load_dataset(self, split, epoch=1, combine=False, **kwargs):
        
    #     # # Get prefix
    #     # prefix = os.path.join(self.args.data, '{}.x-y'.format(split))        
        
    #     """
    #     SOURCE DATA HANDLING
    #     """
        
    #     # Prep source data and the length of the source data
    #     sources = []
    #     lengths = []
        
    #     # # Read source sentences file
    #     # with open(prefix + '.x', encoding='utf-8') as file:
            
    #     #     # Iterate through each line
    #     #     for line in file:
                
    #     #         # Strip the source sentence
    #     #         sentence = line.strip()
                
    #     #         # Tokenize the sentence, splitting on spaces
    #     #         tokens = self.input_vocab.encode_line(
    #     #             sentence, add_if_not_exist=False
    #     #         )
                
    #     #         # Append tokens to the sentences list
    #     #         # and its length to length list
    #     #         sources.append(tokens)
    #     #         lengths.append(len(tokens))

    #     """
    #     TARGET DATA HANDLING
    #     """
        
    #     # Split the paths to the data
    #     paths = utils.split_paths(self.args.data)
    #     assert len(paths) > 0
        
    #     # Get the path splits
    #     data_path = paths[(epoch - 1) % len(paths)]
    #     split_path = os.path.join(data_path, split)
        
    #     # Create dataset instance
    #     tgt_dataset = data_utils.load_indexed_dataset(
    #         split_path, self.dictionary, self.args.dataset_impl, combine=combine
    #     )
        
    #     # If no dataset instance is created, raise an error
    #     if tgt_dataset is None:
    #         raise FileNotFoundError(
    #             "Dataset not found: {} ({})".format(split, split_path)
    #         )
        
    #     # Shorten dataset if need be
    #     tgt_dataset = maybe_shorten_dataset(
    #         tgt_dataset,
    #         split,
    #         self.args.shorten_data_split_list,
    #         self.args.shorten_method,
    #         self.args.tokens_per_sample,
    #         self.args.seed,
    #     )
        
    #     # Create a tupled multihead dataset
    #     tgt_dataset = TupleMultiHeadDataset(
    #         tgt_dataset,
    #         tgt_dataset.sizes,
    #         self.args.tokens_per_sample,
    #         pad=self.dictionary.pad(),
    #         eos=self.dictionary.eos(),
    #         break_mode=self.args.sample_break_mode,
    #         include_targets=True,
    #         ratio=self.args.ratio + 1,
    #         sample_overlap_rate=self.args.sample_overlap_rate,
    #         permutation_invariant=self.args.perm_inv,
    #         evt_vocab_size=self.args.evt_voc_size,
    #         trk_vocab_size=self.args.trk_voc_size,
    #     )
        
    #     """
    #     DATASET COMPILATION
    #     """

    #     # Generate the dataset
    #     self.datasets[split] = LanguagePairDataset(
    #         src=sources,
    #         src_sizes=lengths,
    #         src_dict=self.src_vocab,
    #         tgt=tgt_dataset,
    #         tgt_sizes=tgt_dataset.sizes,
    #         tgt_dict=self.tgt_vocab
    #     )